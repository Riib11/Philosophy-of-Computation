{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to Yann LeCun, “adversarial training is the coolest thing since sliced bread”. I’m inclined to believe so because I don’t think sliced bread ever created this much buzz and excitement within the deep learning community. Generative Adversarial Networks are a set of models that basically learn to create synthetic data that is similar to input data it's given. In more formal terms, a GAN is a generative model that learns the probability distribution (or data distribution) of the training examples it is given. From this distribution, we can then create sample outputs. GANs have seen their largest progress with image training examples, but this idea of modeling data distributions is one that can be applied with other forms of input.In the case described in today’s post, we’ll be creating a GAN that learns to generate synthetic, yet readable, images of MNIST digits. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we'll be explaining generative adversarial networks, and how you can use them to create a generator network that can create realistic MNIST digits through Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s dig a little bit deeper into the structure of this model. The basic idea of these networks is that you have 2 models, a generative model and a discriminative model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![caption](Images/GAN1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The discriminative model has the task of determining whether a given image looks natural (an image from the dataset) or looks like it has been artificially created. This is basically a binary classifier that will take the form of a normal convolutional neural network (CNN).  The task of the generator is to create natural looking images that are similar to the original data distribution. \n",
    "\n",
    "This can be thought of as a zero-sum or minimax two player game. The analogy used in the paper is that the generative model is like “a team of counterfeiters, trying to produce and use fake currency” while the discriminative model is like “the police, trying to detect the counterfeit currency”. The generator is trying to fool the discriminator while the discriminator is trying to not get fooled by the generator. As the models train through alternating optimization, both methods are improved until a point where the “counterfeits are indistinguishable from the genuine articles”. There are specific game theory concepts that prove there is indeed an equilibrium to this game where the generator gets so good that the discriminator outputs a probability of ½ for every input. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today, we’re going to create a GAN that will generate MNIST digits that can fool even the best classifiers (and humans too of course). Here’s what we’re going to need:\n",
    "\n",
    "- Real MNIST training images\n",
    "- A generator network that takes in a random noise vector and produces a synthetic image\n",
    "- A discriminator network (a CNN) that learns to distinguish between real and synthetic images. You can think of it as just a binary classifier (1 for real image, 0 for fake)\n",
    "- An optimization procedure that jointly updates both networks through SGD. This is the tricky part as we need to train the generator network to fool the discriminator network, which means that we have unique gradient flows and labels. \n",
    "- Tensorflow - Our choice of Deep Learning framework\n",
    "\n",
    "Let’s get started. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading MNIST Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's start with our imports. We'll mainly just need Tensorflow. I've also imported Numpy to help with some matrices, the random library for generating numbers, and Matplotlib for visualizing our image data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have to import our MNIST images. To do this, we’ll call a TF function called read_data_sets. This loads in the 55,000 training examples in the MNIST database. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-3-f029be9aacee>:3: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /Users/Henry/anaconda3/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /Users/Henry/anaconda3/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /Users/Henry/anaconda3/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /Users/Henry/anaconda3/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "image_size = 28\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\")\n",
    "# print(len(mnist.train.images[:55000,:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mnist variable we created above actually contains both the images and their labels. Let's just isolate the images for now. There will be 55,000 images and each of them will be of sixe 28 x 28."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55000, 784)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = mnist.train.images[:55000,:]\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at what a random image might look like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "randomNum = random.randint(0,55000)\n",
    "image = x_train[randomNum].reshape([image_size,image_size])\n",
    "plt.imshow(image, cmap=plt.get_cmap('gray_r'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I matched the organization of the MNIST data and just put in my own with its own labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 1953.35it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 2018.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 2 classes: ['white', 'black']\n",
      "loaded 2000 images\n"
     ]
    }
   ],
   "source": [
    "class InputData:\n",
    "    \n",
    "    def __init__(self, training_dir):\n",
    "        self.training_dir = training_dir\n",
    "        \n",
    "        # names of the image classes\n",
    "        self.classes = []\n",
    "        # each image is a 2d array\n",
    "        self.images = []\n",
    "        # each image class is an integer corresponding to the index of the class\n",
    "        self.images_classes = []\n",
    "    \n",
    "    def load_data(self):\n",
    "        # load tagged image data\n",
    "        for directory in os.listdir(self.training_dir):\n",
    "            if directory.endswith('.DS_Store'): continue\n",
    "            classname = directory\n",
    "            self.classes.append(classname)\n",
    "            img_cls = self.classes.index(classname)\n",
    "            for filename in tqdm(os.listdir(self.training_dir+\"/\"+directory)):\n",
    "                if not filename.endswith('.png'): continue\n",
    "                img_arr = np.array(Image.open(self.training_dir+\"/\"+directory+\"/\"+filename), dtype='int32')\n",
    "                self.images.append(img_arr)\n",
    "                self.images_classes.append(img_cls)\n",
    "                    \n",
    "        self.images = np.array(self.images)\n",
    "        self.images_classes = np.array(self.images_classes)\n",
    "        self.num_classes = len(self.classes)\n",
    "        self.num_images = len(self.images)\n",
    "        print(\"loaded {0} classes: {1}\".format(self.num_classes, self.classes))\n",
    "        print(\"loaded {0} images\".format(self.num_images))\n",
    "    \n",
    "    def next_batch(self, batch_size):\n",
    "        images_indecies = random.sample(range(self.num_images), batch_size)\n",
    "        images = np.array([ self.images[i] for i in images_indecies ])\n",
    "        images_classes = np.array([ self.images_classes[i] for i in images_indecies ])\n",
    "        return images, images_classes\n",
    "\n",
    "\n",
    "input_data = InputData('solidcolor_1')\n",
    "input_data.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1c362477f0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(input_data.images[0].shape)\n",
    "plt.imshow(input_data.images[101], cmap='gray_r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discriminator Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   Now, using our classical CS concept of modularity, let’s define a CNN classifier function that takes in an image (of size 28 x 28 x 1) as input. The output will be a single scalar number activation that describes whether or not the input image is real or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![caption](Images/GAN2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to do that, let's first define some functions that will help us with creating CNNs in Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d(x, W):\n",
    "  return tf.nn.conv2d(input=x, filter=W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def avg_pool_2x2(x):\n",
    "  return tf.nn.avg_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let’s see how we’re going to compose this network. We’ll start off by passing the image through a convolutional layer. First, we create our weight and bias variables through tf.get_variable. Our first weight matrix (or filter) will be of size 5x5 and will have a output depth of 8. It will be randomly initialized from a normal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we’ll call the function tf.nn.conv2d() through our a helper function called conv2d. tf.nn.conv2d() is the Tensorflow’s function for a common convolution. It takes in 4 arguments. The first is the input volume (our 28 x 28 x 1 image in this case). The next argument is the filter/weight matrix. Finally, you can also change the stride and padding of the convolution. Those two values affect the dimensions of the output volume. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with any convolutional neural network, this module is repeated, and then followed by a series of fully connected layers. At the end of the network, we do a final matrix multiply and return the activation value. For those of you comfortable with CNNs, this is just a simple binary classifier. Nothing fancy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This architecture for this network is based on Tensorflow's sample CNN classifier model that they have explained in detail here: https://www.tensorflow.org/tutorials/mnist/pros/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator(x_image, reuse=False):\n",
    "    with tf.variable_scope('discriminator') as scope:\n",
    "        if (reuse):\n",
    "            tf.get_variable_scope().reuse_variables()\n",
    "        #First Conv and Pool Layers\n",
    "        W_conv1 = tf.get_variable('d_wconv1', [5, 5, 1, 8], initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "        b_conv1 = tf.get_variable('d_bconv1', [8], initializer=tf.constant_initializer(0))\n",
    "        h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "        h_pool1 = avg_pool_2x2(h_conv1)\n",
    "\n",
    "        #Second Conv and Pool Layers\n",
    "        W_conv2 = tf.get_variable('d_wconv2', [5, 5, 8, 16], initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "        b_conv2 = tf.get_variable('d_bconv2', [16], initializer=tf.constant_initializer(0))\n",
    "        h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "        h_pool2 = avg_pool_2x2(h_conv2)\n",
    "\n",
    "        #First Fully Connected Layer\n",
    "        W_fc1 = tf.get_variable('d_wfc1', [7 * 7 * 16, 32], initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "        b_fc1 = tf.get_variable('d_bfc1', [32], initializer=tf.constant_initializer(0))\n",
    "        h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*16])\n",
    "        h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "\n",
    "        #Second Fully Connected Layer\n",
    "        W_fc2 = tf.get_variable('d_wfc2', [32, 1], initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "        b_fc2 = tf.get_variable('d_bfc2', [1], initializer=tf.constant_initializer(0))\n",
    "\n",
    "        #Final Layer\n",
    "        y_conv=(tf.matmul(h_fc1, W_fc2) + b_fc2)\n",
    "    return y_conv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our discriminator defined, let’s take a look at the generator module. For this, we’ll be basing our model off the generator introduced in the DCGAN paper (link: https://arxiv.org/pdf/1511.06434v2.pdf). You can think of the generator as being a kind of reverse ConvNet. With CNNs, the goal is to transform a 2 or 3 dimensional matrix of pixel values into a single probability. A generator, however, seeks to take a d-dimensional noise vector and upsample it to become a 28 x 28 image. This upsampling is done through a convolutional transpose (or deconvolution) layer. ReLUs and Batch Norm are then used to stabilize the outputs of each layer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The structure of the generator is very similar to that of the discriminator, except we're calling the convolution transpose method, instead of the conv2d one. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The conv transpose + relu + batch norm pipeline is repeated 4 times so that the output volume grows larger and larger until a 28 x 28 x 1 image is formed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(z, batch_size, z_dim, reuse=False):\n",
    "    with tf.variable_scope('generator') as scope:\n",
    "        if (reuse):\n",
    "            tf.get_variable_scope().reuse_variables()\n",
    "        g_dim = 64 #Number of filters of first layer of generator \n",
    "        c_dim = 1 #Color dimension of output (MNIST is grayscale, so c_dim = 1 for us)\n",
    "        s = image_size #Output size of the image\n",
    "        s2, s4, s8, s16 = int(s/2), int(s/4), int(s/8), int(s/16) #We want to slowly upscale the image, so these values will help\n",
    "                                                                  #make that change gradual.\n",
    "\n",
    "        h0 = tf.reshape(z, [batch_size, s16+1, s16+1, 25])\n",
    "        h0 = tf.nn.relu(h0)\n",
    "        #Dimensions of h0 = batch_size x 2 x 2 x 25\n",
    "\n",
    "        #First DeConv Layer\n",
    "        output1_shape = [batch_size, s8, s8, g_dim*4]\n",
    "        W_conv1 = tf.get_variable('g_wconv1', [5, 5, output1_shape[-1], int(h0.get_shape()[-1])], \n",
    "                                  initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "        b_conv1 = tf.get_variable('g_bconv1', [output1_shape[-1]], initializer=tf.constant_initializer(.1))\n",
    "        H_conv1 = tf.nn.conv2d_transpose(h0, W_conv1, output_shape=output1_shape, \n",
    "                                         strides=[1, 2, 2, 1], padding='SAME') + b_conv1\n",
    "        H_conv1 = tf.contrib.layers.batch_norm(inputs = H_conv1, center=True, scale=True, is_training=True, scope=\"g_bn1\")\n",
    "        H_conv1 = tf.nn.relu(H_conv1)\n",
    "        #Dimensions of H_conv1 = batch_size x 3 x 3 x 256\n",
    "\n",
    "        #Second DeConv Layer\n",
    "        output2_shape = [batch_size, s4 - 1, s4 - 1, g_dim*2]\n",
    "        W_conv2 = tf.get_variable('g_wconv2', [5, 5, output2_shape[-1], int(H_conv1.get_shape()[-1])], \n",
    "                                  initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "        b_conv2 = tf.get_variable('g_bconv2', [output2_shape[-1]], initializer=tf.constant_initializer(.1))\n",
    "        H_conv2 = tf.nn.conv2d_transpose(H_conv1, W_conv2, output_shape=output2_shape, \n",
    "                                         strides=[1, 2, 2, 1], padding='SAME') + b_conv2\n",
    "        H_conv2 = tf.contrib.layers.batch_norm(inputs = H_conv2, center=True, scale=True, is_training=True, scope=\"g_bn2\")\n",
    "        H_conv2 = tf.nn.relu(H_conv2)\n",
    "        #Dimensions of H_conv2 = batch_size x 6 x 6 x 128\n",
    "\n",
    "        #Third DeConv Layer\n",
    "        output3_shape = [batch_size, s2 - 2, s2 - 2, g_dim*1]\n",
    "        W_conv3 = tf.get_variable('g_wconv3', [5, 5, output3_shape[-1], int(H_conv2.get_shape()[-1])], \n",
    "                                  initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "        b_conv3 = tf.get_variable('g_bconv3', [output3_shape[-1]], initializer=tf.constant_initializer(.1))\n",
    "        H_conv3 = tf.nn.conv2d_transpose(H_conv2, W_conv3, output_shape=output3_shape, \n",
    "                                         strides=[1, 2, 2, 1], padding='SAME') + b_conv3\n",
    "        H_conv3 = tf.contrib.layers.batch_norm(inputs = H_conv3, center=True, scale=True, is_training=True, scope=\"g_bn3\")\n",
    "        H_conv3 = tf.nn.relu(H_conv3)\n",
    "        #Dimensions of H_conv3 = batch_size x 12 x 12 x 64\n",
    "\n",
    "        #Fourth DeConv Layer\n",
    "        output4_shape = [batch_size, s, s, c_dim]\n",
    "        W_conv4 = tf.get_variable('g_wconv4', [5, 5, output4_shape[-1], int(H_conv3.get_shape()[-1])], \n",
    "                                  initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "        b_conv4 = tf.get_variable('g_bconv4', [output4_shape[-1]], initializer=tf.constant_initializer(.1))\n",
    "        H_conv4 = tf.nn.conv2d_transpose(H_conv3, W_conv4, output_shape=output4_shape, \n",
    "                                         strides=[1, 2, 2, 1], padding='VALID') + b_conv4\n",
    "        H_conv4 = tf.nn.tanh(H_conv4)\n",
    "        #Dimensions of H_conv4 = batch_size x 28 x 28 x 1\n",
    "\n",
    "    return H_conv4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating a Sample Image "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, so now we’ve defined both the generator and discriminator functions. Let’s see what a sample output from an untrained generator looks like. With Tensorflow, we need to first define a session and then create a placeholder for the input to our generator. The purpose of a placeholder is basically to tell Tensorflow \"We're going to input in our random z vector later, but for now, we're going to define this placeholder variable instead\". It lets Tensorflow know about the size of the inputs beforehand. The shape of the placeholder will be None x z_dimensions. The None keyword means that the value can be determined at session runtime. We normally have None as our first dimension so that we can have variable batch sizes (With a batch size of 16, the input to the generator would be 16 x 100). With the None keywoard, we don't have to specify batch_size until later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "z_dimensions = 100\n",
    "z_test_placeholder = tf.placeholder(tf.float32, [None, z_dimensions])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we create a variable (sample_image) that holds the output of the generator, and also initialize the random noise vector that we’ll use as input. The np.random.uniform function has three arguments. The first and second define the range of the output distribution we want (between -1 and 1 in our case), and the third defines the the shape of the vector (1 x 100)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/Henry/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "sample_image = generator(z_test_placeholder, 1, z_dimensions)\n",
    "test_z = np.random.uniform(-1, 1, [1,z_dimensions])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we initialize all the variables, feed our test_z into the placeholder, and run the session. The sess.run function has two arguments. The first is called the \"fetches\" argument. It defines the value for you're interested in computing. For example, in our case, we want to see what the output of the generator is. If you look back at the last code snippet, the output of the generator function is stored in sample_image. Therefore, we'll use sample_image for our first argument. The second argument is where we input our feed_dict. This data structure is where we provide inputs to all of our placeholders. In our example, we need to feed our test_z variable into the z placeholder we defined earlier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())\n",
    "temp = (sess.run(sample_image, feed_dict={z_test_placeholder: test_z}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can view the output through matplotlib. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "my_i = temp.squeeze()\n",
    "plt.imshow(my_i, cmap='gray_r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not a very convincing MNIST digit right? Let’s look at how we can make our generator better. Enter loss functions and optimization!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "tf.reset_default_graph() #Since we changed our batch size (from 1 to 16), we need to reset our Tensorflow graph\n",
    "\n",
    "sess = tf.Session()\n",
    "x_placeholder = tf.placeholder(\"float\"    , shape = [None,image_size,image_size,1]) #Placeholder for input images to the discriminator\n",
    "z_placeholder = tf.placeholder(tf.float32 , [None, z_dimensions]                  ) #Placeholder for input noise vectors to the generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the trickiest parts about understanding GANs is that the loss function is a little bit more complex than that of a traditional CNN classifiers (For those, a simple MSE or Hinge Loss would do the trick). If you think back to the introduction, a GAN can be thought of as a zero sum minimax game. The generator is constantly improving to produce more and more realistic images, while the discriminator is trying to get better and better at distinguishing between real and generated images. This means that we need to formulate loss functions that affect both networks. Let’s take a look at the inputs and outputs of our networks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dx = discriminator (x_placeholder)                           # Dx will hold discriminator prediction probabilities for the real MNIST images\n",
    "Gz = generator     (z_placeholder, batch_size, z_dimensions) # Gz holds the generated images\n",
    "Dg = discriminator (Gz, reuse=True)                          # Dg will hold discriminator prediction probabilities for generated images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, let’s first think about what we want out of our networks. We want the generator network to create images that will fool the discriminator. The generator wants the discriminator to output a 1 (positive example). Therefore, we want to compute the loss between the Dg and label of 1. This can be done through the tf.nn.sigmoid_cross_entropy_with_logits function. This means that the cross entropy loss will be taken between the two arguments. The \"with_logits\" component means that the function will operate on unscaled values. Basically, this means that instead of using a softmax function to squish the output activations to probability values from 0 to 1, we simply return the unscaled value of the matrix multiplication. Take a look at the last line of our discriminator. There's no softmax or sigmoid layer at the end. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reduce mean function just takes the mean value of all of the components in the matrixx returned by the cross entropy function. This is just a way of reducing the loss to a single scalar value, instead of a vector or matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits = Dg, labels = tf.ones_like(Dg))) # ensure forward compatibility: function needs to have logits and labels args explicitly used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let’s think about the discriminator’s point of view. Its goal is to just get the correct labels (output 1 for each MNIST digit and 0 for the generated ones). We’d like to compute the loss between Dx and the correct label of 1 as well as the loss between Dg and the correct label of 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits = Dx, labels = tf.ones_like(Dx)))\n",
    "d_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits = Dg, labels = tf.zeros_like(Dg)))\n",
    "d_loss = d_loss_real + d_loss_fake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have our 2 loss functions (d_loss and g_loss), we need to define our optimizers. Keep in mind that the optimizer for the generator network needs to only update the generator’s weights, not those of the discriminator. In order to make this distinction, we need to create 2 lists, one with the discriminator’s weights and one with the generator’s weights. This is where naming all of your Tensorflow variables can come in handy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tvars = tf.trainable_variables()\n",
    "d_vars = [var for var in tvars if 'd_' in var.name]\n",
    "g_vars = [var for var in tvars if 'g_' in var.name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we specify our two optimizers. In today’s era of deep learning, Adam seems to be the best SGD optimizer as it utilizes adaptive learning rates and momentum. We call Adam's minimize function and also specify the variables that we want it to update. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = tf.train.AdamOptimizer()\n",
    "trainerD = adam.minimize(d_loss, var_list=d_vars)\n",
    "trainerG = adam.minimize(g_loss, var_list=g_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also specify a learning rate by passing it as an argument (I’ve found .0002 to be effective). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ah, now the best part of neural networks, the famous training loop. During every iteration, there will be two updates being made, one to the discriminator and one to the generator. For the generator update, we’ll feed in a random z vector to the generator and pass that output to the discriminator to obtain a probability score (this is the Dg variable we specified earlier). As we remember from our loss function, the cross entropy loss gets minimized, and only the generator’s weights and biases get updated. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll do the same for the discriminator update. We’ll be taking a batch of images from the mnist variable we created way at the beginning of our program. These will serve as the positive examples, while the images in the previous section are the negative ones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 1 --- Discriminator Accuracy: 1.3858683109283447, Discriminator Loss: 1.3862210512161255\n",
      "Training Epoch 2 --- Discriminator Accuracy: 1.148011326789856, Discriminator Loss: 1.1465473175048828\n",
      "Training Epoch 3 --- Discriminator Accuracy: 1.1278038024902344, Discriminator Loss: 1.1486999988555908\n",
      "Training Epoch 4 --- Discriminator Accuracy: 0.6851789951324463, Discriminator Loss: 0.7537218332290649\n",
      "Training Epoch 5 --- Discriminator Accuracy: 0.6741829514503479, Discriminator Loss: 0.6781442165374756\n",
      "Training Epoch 6 --- Discriminator Accuracy: 0.4037741422653198, Discriminator Loss: 0.47002100944519043\n",
      "Training Epoch 7 --- Discriminator Accuracy: 0.6841537952423096, Discriminator Loss: 0.47912704944610596\n",
      "Training Epoch 8 --- Discriminator Accuracy: 0.5108691453933716, Discriminator Loss: 0.38021859526634216\n",
      "Training Epoch 9 --- Discriminator Accuracy: 0.3686383366584778, Discriminator Loss: 0.3456605076789856\n",
      "Training Epoch 10 --- Discriminator Accuracy: 0.3158964216709137, Discriminator Loss: 0.32681363821029663\n",
      "Training Epoch 11 --- Discriminator Accuracy: 0.5913692116737366, Discriminator Loss: 0.4616985023021698\n",
      "Training Epoch 12 --- Discriminator Accuracy: 0.2382035255432129, Discriminator Loss: 0.23667234182357788\n",
      "Training Epoch 13 --- Discriminator Accuracy: 0.6835750937461853, Discriminator Loss: 0.601750373840332\n",
      "Training Epoch 14 --- Discriminator Accuracy: 0.2919648587703705, Discriminator Loss: 0.3171854615211487\n",
      "Training Epoch 15 --- Discriminator Accuracy: 1.673633337020874, Discriminator Loss: 1.0959185361862183\n",
      "Training Epoch 16 --- Discriminator Accuracy: 0.6139073967933655, Discriminator Loss: 0.601949155330658\n",
      "Training Epoch 17 --- Discriminator Accuracy: 0.6282173991203308, Discriminator Loss: 0.6616174578666687\n",
      "Training Epoch 18 --- Discriminator Accuracy: 0.30946725606918335, Discriminator Loss: 0.33595168590545654\n",
      "Training Epoch 19 --- Discriminator Accuracy: 0.626539945602417, Discriminator Loss: 0.5467140674591064\n",
      "Training Epoch 20 --- Discriminator Accuracy: 0.381131649017334, Discriminator Loss: 0.4128069579601288\n",
      "Training Epoch 21 --- Discriminator Accuracy: 0.4620662331581116, Discriminator Loss: 0.5464296936988831\n",
      "Training Epoch 22 --- Discriminator Accuracy: 0.7082661986351013, Discriminator Loss: 0.6490808129310608\n",
      "Training Epoch 23 --- Discriminator Accuracy: 0.5744184255599976, Discriminator Loss: 0.5304356813430786\n",
      "Training Epoch 24 --- Discriminator Accuracy: 0.7053118348121643, Discriminator Loss: 0.6056525111198425\n",
      "Training Epoch 25 --- Discriminator Accuracy: 0.7357556819915771, Discriminator Loss: 0.7896009683609009\n",
      "Training Epoch 26 --- Discriminator Accuracy: 0.7180781364440918, Discriminator Loss: 0.7650985717773438\n",
      "Training Epoch 27 --- Discriminator Accuracy: 0.4330812990665436, Discriminator Loss: 0.5214704275131226\n",
      "Training Epoch 28 --- Discriminator Accuracy: 0.748496949672699, Discriminator Loss: 0.6808249950408936\n",
      "Training Epoch 29 --- Discriminator Accuracy: 0.7587422132492065, Discriminator Loss: 0.714088499546051\n",
      "Training Epoch 30 --- Discriminator Accuracy: 0.6729063987731934, Discriminator Loss: 0.678203821182251\n",
      "Training Epoch 31 --- Discriminator Accuracy: 1.2011529207229614, Discriminator Loss: 0.8692320585250854\n",
      "Training Epoch 32 --- Discriminator Accuracy: 0.7947554588317871, Discriminator Loss: 0.778868556022644\n",
      "Training Epoch 33 --- Discriminator Accuracy: 1.0226593017578125, Discriminator Loss: 1.0518008470535278\n",
      "Training Epoch 34 --- Discriminator Accuracy: 1.0560815334320068, Discriminator Loss: 1.1464018821716309\n",
      "Training Epoch 35 --- Discriminator Accuracy: 0.763278067111969, Discriminator Loss: 0.8228657841682434\n",
      "Training Epoch 36 --- Discriminator Accuracy: 0.6943932771682739, Discriminator Loss: 1.6196800470352173\n",
      "Training Epoch 37 --- Discriminator Accuracy: 0.7697820663452148, Discriminator Loss: 0.8021367788314819\n",
      "Training Epoch 38 --- Discriminator Accuracy: 0.8884381651878357, Discriminator Loss: 0.8194829821586609\n",
      "Training Epoch 39 --- Discriminator Accuracy: 1.113037347793579, Discriminator Loss: 0.8943606615066528\n",
      "Training Epoch 40 --- Discriminator Accuracy: 0.7733614444732666, Discriminator Loss: 0.6895489692687988\n"
     ]
    }
   ],
   "source": [
    "# initialize\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# log progress\n",
    "def show_progress(epoch, feed_dict_D):\n",
    "    msg = \"Training Epoch {epoch} --- Discriminator Accuracy: {dAcc}, Discriminator Loss: {dLoss}\"\n",
    "    # dAcc = sess.run(d_loss_real, feed_dict=feed_dict_D)\n",
    "    dAcc = sess.run(d_loss, feed_dict_D)\n",
    "    print(msg.format(epoch=epoch, dAcc=dAcc, dLoss=dLoss))\n",
    "\n",
    "# training\n",
    "iterations = 2000\n",
    "epoch_size = 50\n",
    "for i in range(iterations):\n",
    "    \n",
    "    z_batch = np.random.uniform(-1, 1, size=[batch_size, z_dimensions])\n",
    "    \n",
    "    real_image_batch = input_data.next_batch(batch_size)\n",
    "    real_image_batch = np.reshape(real_image_batch[0],[batch_size,image_size,image_size,1])\n",
    "    \n",
    "    feed_dict_D = { z_placeholder:z_batch, x_placeholder:real_image_batch}\n",
    "    feed_dict_G = { z_placeholder:z_batch }\n",
    "    \n",
    "    _, dLoss = sess.run( [trainerD,d_loss] , feed_dict=feed_dict_D ) # Update the discriminator\n",
    "    _, gLoss = sess.run( [trainerG,g_loss] , feed_dict=feed_dict_G ) # Update the generator\n",
    "    \n",
    "    if i % epoch_size == 0: show_progress(1+i//epoch_size, feed_dict_D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now see what a sample image looks like after training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"strided_slice_3:0\", shape=(28, 28), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1c39e96fd0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_image = generator(z_placeholder, 1, z_dimensions, reuse=True)\n",
    "print(sample_image[0][:,:,0])\n",
    "z_batch = np.random.uniform(-1, 1, size=[1, z_dimensions])\n",
    "temp = sess.run(sample_image, feed_dict={z_placeholder: z_batch})\n",
    "my_i = temp.squeeze()\n",
    "plt.imshow(my_i, cmap='gray_r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Difficulties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One note that I’d like to make is that GANs are notoriously difficult to train. Without the right hyperparameters, network architecture, and training procedure, there is a high chance that either the generator or discriminator will overpower the other. A common case of this is the situation where the generator is able to find a flaw in the discriminator by repeatedly outputting an image that fits the data distribution the discriminator is looking for, but is nowhere close to being a readable MNIST digit. The generator has collapsed onto a single point, and therefore we won’t output a variety of digits. There are also cases where the discriminator becomes too powerful and is able to easily make the distinction between real and fake images. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mathematical intuition behind this phenomenon lies in that GANs are typically trained using gradient descent techniques that are designed to find the minimum value of a cost function, rather than to find the Nash equilibrium of a game. When used to seek for a Nash equilibrium, these algorithms may fail to converge. Further research into game theory and stable optimization techniques may result in GANs that are as easy to train as ConvNets!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Closing Thoughts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this post, we showed how two networks were able to play a minimax game in order to capture the data distribution of the MNIST digits and generate similar looking samples. With applications in video frame prediction, text-image mappings, and more, GANs are definitely the hottest topic in deep learning. Hopefully, with this tutorial, you’ve gained a better understanding of how these networks work in practice and how you can build your own with Tensorflow!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some more great GAN implementations\n",
    "\n",
    "DCGAN Tensorflow Implentation: https://github.com/carpedm20/DCGAN-tensorflow\n",
    "\n",
    "Arthur Juliani's GAN Implementation: https://github.com/awjuliani/TF-Tutorials/blob/master/DCGAN.ipynb \n",
    "\n",
    "Brandon Amos's Image Completion Project: https://bamos.github.io/2016/08/09/deep-completion/ "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
